"""
TITLE: pyMS-Vis.py

INPUT: msalign file(s) from TopFD and/or tsv file(s) from FLASHDeconv. optional input file type is ms1ft (feature map) generated by
       FLASHDeconv* 
*must be in same folder as application

OUTPUT: interactive quant graphs (.html), quantiative data (.csv)

DESCRIPTION: This application takes multiple deconvoluted files generated by FLASHDeconv (tsv) or TopFD (msalign) from top-down mass spectra and querries this data using user inputs.
The app plots the mass vs. abundance of all found masses, averages of their relative intensities, and a feature map. User input is straightforward & minimal including:
   - Users will be prompted to upload msalign or tsv files (at least one is required).
   - Each file can be categorized by sample type (e.g. "control group", "stage 1", etc.) in app immediately after uploading. These labels help distinguish between samples on the graph legend. A default label ("exp. group") is provided.
   - Users are prompted to specify individual masses (or a mass range), retention times, etc that they are interested in seeing plotted. Default values are provided.
The interactive output graph (with a quality control graph for each file uploaded) is generated in browser, where users can further inspect & compare the samples' data. A csv file containing the same information as the main graph is saved to the same folder as the application and input files.
"""

import timeit
    # used to determine function runtime while editing & testing
import pandas as pd
    # data anlysis toolkit
        # used to convert data table (from reading .csv file) into NumPy matrix array in
        # df_feat_map = pd.read_csv(file_path)
import numpy as np
    # numpy: for working with arrays
    # append, reshape, array
import tkinter as tk
    # tkinter: for Tcl/Tk GUI toolkit (standard, n  ot necessarily best?)
    # Frame, Button, Label, Entry, Checkbutton
import time, sys    # -----------------------------------------------------------------> import time?
    # time: for time-related functions, such as strptime() used in output_quant_file and qc_graph
    # sys: to access system-specific parameters and functions, i/o
        # seemse like it's only being used for one output line, which is commented out
import matplotlib                               
import matplotlib.pyplot as plt
    # pyplot: makes matplotlib work like matlab, used for plotting qc graphs
import matplotlib.backends._tkagg
matplotlib.use("TkAgg")
    # backends._tkagg: to use matplotlib with tkinter, more imports from here below
from matplotlib import style
style.use('ggplot')
    # style: used to customized appearance of plots,
    # ggplot: a style that emulates the ggplot package for R
import os, os.path 
    # os: "provides a portable way of using operation system dependent functionality"
    # i'm guessing these are just used to access the date&time on the computer, for plotting?
import statistics
    # used in calculate_avg_stdev() in QuantOutput
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2Tk
    # FigureCanvasTkAgg: interface between the Figure and Tkinter Canvas
    # NavigationToolbar2Tk: built-in toolbar for the figure on the graph
    # these make the graph interactive as desired
    # in makegraph(), specifications for toolbar are passed in as parameters
from matplotlib.figure import Figure
    # Figure: contains all plot elements, used to control spacing of subplots & container
    # needed for multiple qc graphs in case of multiple msalign file uploads
from operator import itemgetter, attrgetter         # not sure how/where these are used
    # itemgetter: constructs a callable that assumes an iterable object and returns the nth element
    # attrgetter: returns a callable object that fetches one/more attributes from its operand
from datetime import datetime
    # datetime: such as datetime, used in output_quant_file and qc_graphs
from tkinter import filedialog as fd                                    # REMOVE
    # filedialog: provides a set of dialogs to use when working with file, such as open, save, etc.
    # used askopenfilename once in deconv_file() in FileSelection
from tkinter import messagebox as mb
    # messagebox: provides a template base class for messageboxes
    # used to display error messages during file uploads
from tkinter import ttk
    # ttk: provides acces to Tk themed widge set - buttons, etc.
from tkinter import *
from tkinter import filedialog
    # filedialog: provides a set of dialogs to use when working with file, such as open, save, etc.
    # used to upload & open ms1ft .csv files                     
from collections.abc import Iterable
    # Iterable: provides abstract base classes that can be used to test whether a class
    # provides a particular interface, so like issubclass() or isinstance()
from bokeh.models import ColumnDataSource, Whisker, HoverTool, Legend, LegendItem, Tabs, Div, BasicTicker, ColorBar, CustomJS, TapTool, LogColorMapper, PrintfTickFormatter, TabPanel, Dropdown
    # ColumnDataSource: provides data to the glyphs of plot so you can pass in lists, etc.
    # Whisker: adds a whisker for error margins
    # HoverTool: passive inspector tool, for actions to occur when cursor hovering
    # Legend: allows us more advanced control of the legend object provided by bokeh for the graphs
    # LegendItem: set True to have legend visible, False to hide legend
    # Tabs and Panel: Lets you use panels and tabs for graphs
    # Div: corresponds to an HTML <div> element
from bokeh.plotting import figure, show, curdoc
    # figure: a function that creates a Figure model, which composes axes, grids, default tools, etc.
    # and includes methods for adding different kinds of glyphs (shapes & things) to a plot
    # show: displays the figure
    # curdoc:
from bokeh.layouts import gridplot, column, row, layout
    # gridplot: a function which arranges bokeh plots in a grid and merges all plot tools into a single
    # toolbar so that each plot int he grid has the same active tool
from bokeh.io import output_file
    # output_file: configures the default output state to generate output saved to a file when show()
    # is called
from bokeh.palettes import Category20b, Category20c
    # Category20b: 4 purple, 4 green, 4 brown, 4 red, 4 pink
    # Category20c: 4 blue, 4 orange, 4 green, 4 purple, 4 grey
from bokeh.transform import dodge, transform
    # dodge: creates a DataSpec dict that applies a client-side Dodge transformation to a
    # ColumnDataSoure column. has parameters:
        # field_name (str) - a field name to configure DataSpec with
        # value (float) - the fixed offset to add to column data
        # range (Range, optional) - a range to use for computing synthetic coordinates when necessary,
        # e.g. a FactorRange when the column data is categorical (not the fun kind)
from bokeh.resources import INLINE
from bokeh.events import Tap
from idlelib.tooltip import Hovertip
import math
from functools import reduce

LARGE_FONT= ("Verdana", 12)

"""
APP

This builds the backbone of the app using tkinter. Each page of the app is a frame in App.frames. Throughout the code, pages are displayed using App.show_frame('pagename'). The first page is set here as well, and is set to the 'File Selection' page.


"""
class App(tk.Tk):   
    deconv_filearray = []
    processed_filearray = []
    shortened_filenames = []
    expgroup = []
    feat_map_filenames = []
    total_files = 0

    def __init__(self, *args, **kwargs):
        # initializing App
        tk.Tk.__init__(self, *args, **kwargs)
        tk.Tk.wm_title(self,"pyMS-Vis: A Tool for Quantification and Visualization of MS1 Data")

        # create the Frame the App is contained in (container for App)
        container = tk.Frame(self)
        container.grid(column=0, row=0, sticky='news')
        container.grid_rowconfigure(0, weight=1)
        container.grid_columnconfigure(0, weight=1)
        self.geometry("1280x740")                               # size of window (width x height) in pixels

        # creates the frames (pages in the App)
        self.frames = {}
        for F in (FileSelection, SearchParams, QuantOutput, QCGraphs):
            frame = F(container, self)                          # creates a frame for each page
            self.frames[F] = frame                              # adds to App's array self.frames
            frame.grid(row=0, column=0, sticky="news")

        # app opens with File Selection
        self.show_frame(FileSelection)                             

    # show_frame(): shows a page of the app (usually called when a button is pressed, except above)
    def show_frame(self, cont):
        frame = self.frames[cont]
        frame.tkraise()

    # RMV_CHARS(): takes a string and returns a string of only the numbers and periods. 
    # This function will be called on lines gathered from input msalign files, and is intended to filter out headings/labels to gather data for calculations. 
    # Ex: rmv_chars('RETENTION_TIME=13.20') = '13.20'
    def rmv_chars(string):
        getVals = list([val for val in string
            if (val.isnumeric() or val==".")])
        return "".join(getVals)

"""
FILE SELECTION

Clicking "MS1 File Selection" on the 'Start Page' leads to the 'MS1 File Selection' page, which has two buttons, one for "New Analysis" and one for "Add File". 
Once one file has been uploaded, a button to "Process Files" appears, created within deconv_file().

"""
class FileSelection(tk.Frame):
    gridrow = 5 # this must be increased if additional labels or buttons are added ABOVE where the selected file labels show up.
    gridcolumn = 0
    button_identities = []
    filelabel_identities = []
    group_identities = []
    remove_button_start = 2 #this MUST BE CHANGED if additional buttons are added/removed to the FileSelection window in __init__. The method Xclick() uses this to start counting 'remove' buttons associated with each file selection.

    def __init__(self, parent, controller):
        # File Selection Page creation & initialization
        tk.Frame.__init__(self, parent)
        label = tk.Label(self, text="Deconvoluted MS1 File Selection", font=LARGE_FONT)
        label.grid(column=0, row=0, sticky='w')
               
        # Add Feature Map Selection Box
        # Feature Map Options: None, Feature, MS1FT, *Manual*
        # 'None' is default. 'Feature' are '.feature' files from TopFD, 'MS1FT' are '.ms1ft' files from Promex/FLASHDeconv, '*Manual*' is manual selection, and 'None' lets the user opt-out 
        option_list = ['None','.feature','.ms1ft','*Manual*']
        self.map_optn = StringVar(self)          # map_optn selects map
        self.map_optn.set('None')              # default is nothing, which forces the user to select an option prior to loading deconv files
        self.popupMenu = OptionMenu(self, self.map_optn, *option_list)

        # Menu label for Search Mode
        self.map_type = tk.Label(self, text="Feature Map Type:")
        self.map_type.grid(column = 0, row = 2, sticky='e')
        self.popupMenu.grid(column = 1, row = 2)
        self.popupMenu_tip = Hovertip(self.popupMenu, 'Automatically load feature maps of the corresponding \ndeconvoluted files if "*.feature" files from TopFD \nor "*.ms1ft" from Promex/FLASHDeconv are present. \
                                      \n "*Manual*" is manual file selection and "None" is for no map.', hover_delay=100)
        
        # Add File Button
        button1 = tk.Button(self, text='Add File',  fg="green", command=(lambda : self.deconv_file()))
        button1.grid(column=0, row=3,sticky='w')
        button1_tip = Hovertip(button1, 'Upload a file\n(.msalign or .tsv)', hover_delay=100)

        # Add labels for file name and group name
        label2 = tk.Label(self, text="File Name:", font=LARGE_FONT)
        label2.grid(column=0, row=4, sticky='w')
        label3 = tk.Label(self, text="Group Name:", font=LARGE_FONT)
        label3.grid(column=1, row=4, sticky='w')

        self.controller = controller


    # DECONV_FILE(): uploads a new tsv or msalign file and builds/updates File Selection Page throughout upload process.
    # This function is called each time the "Add File" button is pressed.
    def deconv_file(self):
        if App.total_files > 30:                                                          
            mb.showerror("Warning","Loading more than 30 files may take a while to process.") 

        # timing start
        start_time = timeit.default_timer()
            
        # Retrieve file path from upload
        file_path = filedialog.askopenfilenames(title="Please select MS1 msAlign or tsv file(s)", filetypes=[('All files','*.*')])
        for file in file_path:
            # The abbreviated file name 'filename.ext' of the uploaded file is extracted from its path '.../.../.../.../filename.ext'
            filename = str(file)[::-1]        # takes the uploaded file path as a string, reverses it, and saves this to the variable filename
            for i in range(len(filename)):
                if filename[i] == '/':
                    ext_filename = filename[0:i][::-1]    # takes the reversed file path up to the first '/', reversed, and saves this to the variable ext_filename
                    break
            #App.shortened_filenames.append(ext_filename)
            
            # Once a file has been successfully uploaded, display in app while:
                # allowing user to relabel each file, usually by group (e.g. 'exp group', 'ctrl group')
                # allowing user remove individual uploaded files with a button
            if ext_filename != "":
                # displays uploaded file information
                filelabel = Label(self, text = ext_filename, anchor="w", width=len(ext_filename)+5)  
                filelabel.grid(row=FileSelection.gridrow, column=FileSelection.gridcolumn, sticky='w')

                # creates editable group_label_entry, so that the user can specify what each msalign file represents (control, experiment 1, etc) in app
                # these will be the labels shown on the output graphs
                group_label_entry = tk.Entry(self, width=13)
                group_label_entry.grid(row=FileSelection.gridrow, column=int(FileSelection.gridcolumn)+1, sticky='w')
                group_label_entry.insert(0,'<exp. group>')

                # creates a removebutton alongside & specific to each uploaded file using a lambda that calls the function "Xclick", defined below this fn
                # removebutton variable can't be passed to XClick because it hasn't been definted yet, so the name of the button needs to be passed. Tkinter
                # starts at 2 (depends on number of buttons added to FileSelection) and will add 'X' buttons as files enter or leave the program, which is counted by remove_button_start.
                removebutton_name = '.!frame.!fileselection.!button'+str(self.remove_button_start)
                self.remove_button_start+=1
                removebutton = tk.Button(self, text='X', fg="red", command=(lambda filelabel=filelabel,group_label_entry=group_label_entry,removebutton_name=removebutton_name: Xclick(filelabel,group_label_entry,removebutton_name)))
                removebutton.grid(row=FileSelection.gridrow, column=int(FileSelection.gridcolumn)+2)
                removebutton_tip = Hovertip(removebutton, 'Remove ' + ext_filename, hover_delay=100)

                # changes to FileSelection page
                FileSelection.filelabel_identities.append(filelabel)
                FileSelection.group_identities.append(group_label_entry)
                FileSelection.button_identities.append(removebutton)
                FileSelection.gridrow += 1
                App.total_files+=1

                # update main array by appending file Label and string 
                # deconv_filearray = [Label1, string1, Label2, string2, ...]
                App.deconv_filearray.append(filelabel)
                App.deconv_filearray.append(str(file))

            
            # only show "Process Files" button once there are files to process
            # pressing the "Process Files" button calls the lambda populate_entries()
            if len(App.deconv_filearray) == 2:         # if something uploaded, deconv_file array should have 2 vals (file label & name)
                # spacing around process files button
                space = tk.Label(self, text="\n")
                space.grid(column=0, row=20, sticky='w')
                
                # process files button
                processbutton = tk.Button(self, text='Process File(s)', name='processbutton', bg="white", fg="black", command=(lambda : populate_entries(self)))
                processbutton.grid(column=0, row=99, sticky='w')
                process_tip = Hovertip(processbutton, 'Warning! Once processed, files cannot be changed unless program is restarted.', hover_delay=100)

        # timing end
        total_time = timeit.default_timer() - start_time
        print('Fileselection \t deconv_file \t\t\t', total_time)
        # with open("timing_0_deconv_file.csv", "a") as out_file:         # timing output file for testing
            # out_file.write(str(total_time))
            # out_file.write("\n")


        # XCLICK(): destroys all info for a selected file in FileSelection page (filelabel, entry box, removebutton) and the corresponding values in the identity arrays for those 3 values.
        # Then destroys its 2 values in deconv_filearray and updates the app's total number of files, removing the "Process Files" button if this is 0. The result is as though the selected file was never uploaded.
        def Xclick(filelabel, group_label_entry, removebutton):
            # timing start
            start_time = timeit.default_timer()

            # destroy file's boxes on File Selection page
            filelabel.destroy()
            group_label_entry.destroy()
            self.nametowidget(removebutton).destroy()
            
            # destroy file's vals in identity arrays
            FileSelection.filelabel_identities.remove(filelabel)
            FileSelection.group_identities.remove(group_label_entry)
            
            # remove file label and name from deconv_filearray
            for i in App.deconv_filearray:
                if i == filelabel:
                    del App.deconv_filearray[App.deconv_filearray.index(i)+1]
                    del App.deconv_filearray[App.deconv_filearray.index(i)]

            # update total # of files, if 0 remove "Process Files" button
            App.total_files-=1
            if len(App.deconv_filearray) == 0:
                self.nametowidget('processbutton').destroy()

            # timing end
            total_time = timeit.default_timer() - start_time
            print('FileSelection \t Xclick \t\t\t', total_time)
            # with open("timing_0_Xclick.csv", "a") as out_file:          # timing output file for testing
                # out_file.write(str(total_time))
                # out_file.write("\n")


        # POPULATE_ENTRIES(): fills deconv_filearray with data from group_identities array.
        # Called after all input files are uploaded and 'Process Files' button is pressed.
        def populate_entries(self):
            # timing start
            start_time = timeit.default_timer()
            print('deconv_filearray',App.deconv_filearray)
            
            map_type_sel = self.map_optn.get()

            # Fill expgroup (array) with group names (e.g. 'exp. group')
            # parse through group_identities, one per file but many can hold same string val
            for group in self.group_identities:     
                group_name = group.get()            # group_label_entry in deconv_file()
                App.expgroup.append(group_name)     # ^ these fill prev empty array expgroup

            # extracts filenames to abrv_filenames for graphing display
            # deconv_filearray consists of, for each file, the Label and name, so skip by 2
            for i in range(1,len(App.deconv_filearray),2):     # len(...) = # of input files
                SearchParams.abrv_filenames.append(App.deconv_filearray[i])

                # The abbreviated file name 'filename.ext' of the uploaded file is extracted from its path '.../.../.../.../filename.ext'
                filename = str(App.deconv_filearray[i])[::-1]        # takes the uploaded file path as a string, reverses it, and saves this to the variable filename
                for j in range(len(filename)):
                    if filename[j] == '/':
                        ext_filename = filename[0:j][::-1]    # takes the reversed file path up to the first '/', reversed, and saves this to the variable ext_filename
                        break
                App.shortened_filenames.append(ext_filename)

                #make feature map filename to search and display when graphing
                root_ext = os.path.splitext(App.deconv_filearray[i])
                print('map type sel and root_ext',map_type_sel, root_ext)
                if map_type_sel == '.feature':
                    map_filename = root_ext[0]+'.feature'
                elif map_type_sel =='.ms1ft':
                    map_filename = root_ext[0][:-4]+'.ms1ft'
                elif map_type_sel == '*Manual*':
                    map_filename = str(filedialog.askopenfilename(title="Please select the feature map corresponding to "+ext_filename, filetypes=[('All files','*.*')]))
                else:
                    map_filename = ''
                print('map_filename',map_filename)
                App.feat_map_filenames.append(map_filename)

            # Get a single max ret time and scan time among all files loaded so that 
            # these values are used as default values for Static mode
            for filename in SearchParams.abrv_filenames:
                root_ext = os.path.splitext(filename)
                print('root',root_ext[0])
                n=200 #stops if there are more than 200 lines without any ID or Retention Time text
                pos = n+1
                lines = []
                temp_array = [0,0]
                if root_ext[1] == '.msalign':
                    with open(filename, "rb") as file:
                        while len(lines) <= n and temp_array[0] == 0 or temp_array[1] == 0:
                            try:
                                file.seek(-pos,os.SEEK_END)
                                text = file.readline().decode()
                                if text.startswith('SCANS='):
                                    temp_array[0] = int(App.rmv_chars(text))        # append id no. to temp array
                                if text.startswith('RETENTION_TIME='):
                                    temp_array[1] = float(App.rmv_chars(text)) 
                            except OSError:
                                file.seek(0)
                                print('exception!')
                                break
                            finally:
                                lines = list(file)
                            pos+=1
   
                if root_ext[1] == '.tsv':
                    req_columns = ['ScanNum','RetentionTime']
                    df_tsv = pd.read_csv(filename,usecols=req_columns,sep='\t')
                    temp_array[0] = df_tsv['ScanNum'].max()
                    temp_array[1] = df_tsv['RetentionTime'].max()

                if temp_array[0] > SearchParams.static_scan_max:
                    SearchParams.static_scan_max = temp_array[0]
                    SearchParams.entries[1][1].delete(0, END) 
                    SearchParams.entries[1][1].insert(0,temp_array[0])       # End scan
                if temp_array[1] > SearchParams.static_retention_max:
                    SearchParams.static_retention_max = temp_array[1]
                    SearchParams.entries[3][1].delete(0, END) 
                    SearchParams.entries[3][1].insert(0,temp_array[1])      # End ret. time

                
            # timing end
            total_time = timeit.default_timer() - start_time
            print('FileSelection \t populate_entries \t\t', total_time)
            # with open("timing_populate_entries.csv", "a") as out_file:      # timing output file for testing
            #     out_file.write(str(total_time))
            #     out_file.write("\n")

            # When done processing input files, show 'Search Parameters' page
            self.controller.show_frame(SearchParams)


"""
SEARCH PARAMS

Clicking "Search Parameters" on the 'Start Page' leads to the 'Search Parameters' page, which defaults to Static mode, where the user can specify the values of 6 parameters. Switching to Dynamic mode allows the user to specify different start and end retention times for each file. Also, in Dynamic mode, the user has the option to 'search by mass range' rather than specifying masses of interest by listing them.

"""
class SearchParams(tk.Frame):
    abrv_filenames = []             # abbreviated filenames for display purposes
    entries = []
    dynamic_entries = []
    dynamic_counter = 1             # this will be used for file-specific search conditions (e.g., retention times)
    static_retention_max = 0    # this will provide a default value and change based on the max ret time found across all input files 
    static_scan_max = 0          # this will provide a default value and change based on the max scan number found across all input files 
    
    def __init__(self, parent, controller):
        # Search Parameters Page creation & initialization
        tk.Frame.__init__(self, parent)
        top_frame = tk.Frame(self)
        top_frame.grid(row=0, column=0, columnspan=4, sticky='news')
        label = tk.Label(top_frame, text="Search Parameters",  font=LARGE_FONT)
        label.grid(row=0, column=0, sticky='w')

        # Search Modes: Static, Dynamic
        option_list = ['Static','Dynamic']
        self.search_optn = StringVar(self)          # search_optn changes mode
        self.search_optn.set('Static')              # default is Static
        self.popupMenu = OptionMenu(top_frame, self.search_optn, *option_list)

        # Menu label for Search Mode
        self.search_mode = tk.Label(top_frame, text="Search Mode:")
        self.search_mode.grid(row = 0, column = 3, sticky='e')
        self.search_mode_tip = Hovertip(self.search_mode, 'Static Mode is usually recommended for the first run. \nUse Dynamic Mode to specify retention time per file \nand masses of interest by range.', hover_delay=100)
        self.popupMenu.grid(row = 0, column = 4)
        self.search_optn.trace('w',self.get_parameters)

        # Mass Range Entry
        self.entry_frame = tk.Frame(self)
        self.entry_frame.grid(row=3, column = 0)
        self.mass_range = IntVar()
        self.v_but = IntVar()        # variable to detect radio button selection for ppm or Da mass tolerance
        self.v_but.set(2)

        # Mass tolerance frame
        self.mass_tol_frame = tk.Frame(self)
        self.mass_tol_frame.grid(row=99, column = 0, sticky='w')

        # Mass range frame
        self.mass_range_frame = tk.Frame(self)

        # Pre-populate search options w/(default static) parameters
        self.controller = controller
        self.get_parameters(self)

        # empty space before process files button
        space = tk.Label(self, text="\n")
        space.grid(row=7, column=0, sticky='w')

        # 'Process Files' Button calls process_deconv_files()
        processbutton = tk.Button(self, text='Process File(s)', name='pbutton', bg="white", fg="black", command=(lambda : self.process_deconv_files()))
        processbutton.grid(row=100, column=0, sticky='w')
        process_tip = Hovertip(processbutton, 'Process file(s) according to specified search parameters', hover_delay=500)

        # Set up for progress()
        self.e=StringVar()
        self.e.set("Loading")

    # GET_PARAMETERS(): Displays user entry fields for user specified parameters such as scan start and end time, mass tolerance, etc. Contains two modes - Static (the default mode) and Dynamic. 
    # In Static mode, the user specifies masses of interest by entering a list (separated by spaces or commas and spaces) and a mass tolerance (ppm or Da). The user specifies a single start & end retention time for all files.
    # In Dynamic mode, the user can set masses of interest by range instead of inputting a list, by specifying the minimum and maximum mass values and the interval between them.
    # The program uses these three parameters to create a list of masses of interest. The user specifies mass tolerance as in Static mode. In this mode, the user can specify different start & end retention times for different files.
    def get_parameters(self,*args):
        # Reset entries (for starting or switching back to Search Params page)
        # In Static mode, entries = [('Start Scan', Entry), ('End Scan', Entry), ('Start ret. time', Entry), ('End ret. time', Entry), ('Masses', Entry), ('Mass Tolerance (Da)', Entry)]
        # In Dynamic mode, entries = [('Masses', Entry, Entry, Entry), ('Mass Error Tolerance', Entry)]
        # In Dynamic mode, dynamic_entries = ['filename.ext', ('Start ret. time', Entry), ('End ret. time', Entry), ...], where this info is provided per input file
        SearchParams.entries = []
        SearchParams.dynamic_entries = []
        event = None

        """ ------------------- CHANGEABLE PARAMETERS START HERE ------------------- """
        """                        (Pesavento Lab use Chlamydomonas histone masses)  """
        # The code below determines the default parameters with regards to the masses of interest in both Static and Dynamic mode.
        # To replace these with your own values, follow the steps below. Add a list containing your masses of interest, then edit the default_masses variable to refer to your new list. 

        # STEP 1/2: List your masses of interest below (give your list a name like H3 then list the masses between brackets and separated by commas, as shown).
        H3 = [15168,15182,15196,15210,15224,15238,15252,15266,15280,15294,15308,15322,15336,15350,15364,15378,15392,15406,15430,15444,15458,15472,15486,15500]
        H2A = [13488,13514,13530,13545,13572,13587,13629,13700,13713,13742,13755,13784,13797]
        # H4 = [11318,11332,11346,11360,11374,11388,11402,11416,11430,11444,11458,11472,11486,11500,11514,11528,11542,11556]
        # H4ox = [11334, 11348, 11362,11376,11390,11404,11418,11432,11446,11460,11474,11488,11502,11516,11530,11544,11558]

        # Static mode default values (changeable)
        default_masses = H2A                      # STEP 2/2: Replace H2A in this line with the name of your list.
        default_tolerance = 1                     # Error defaults to Dalton and 1 Da should be fine to start
        default_scan_start = 1
        default_scan_end = self.static_scan_max
        default_ret_start = 0
        default_ret_end = self.static_retention_max
        """ -------------------- CHANGEABLE PARAMETERS END HERE -------------------- """

        # Dynamic mode default values are calculated based on Static mode default values (NOT recommended to change)
        default_min = min(default_masses)
        default_max = max(default_masses)
        default_interval = (2*default_tolerance)+1

        #this provides real-time adjusting tkinter Entry widget width
        def resizer(event,parent_widget):
            if parent_widget.winfo_class() == 'Entry':
                parent_widget.configure(width=len(parent_widget.get())+5)
            else:
                children_widgets = parent_widget.winfo_children()
                for child_widget in children_widgets:
                    if child_widget.winfo_class() == 'Entry':
                        child_widget.configure(width=len(child_widget.get())+5)

        # CHANGE_RANGE(): manages updates to the app when user switches mass search mode between list and range by updating, creating, and destroying fields, entries, and tips.
        def change_range(self,*args,):
            if self.mass_range.get() == 1:              # if "Search by mass range" selected:
                self.mass_field.config(text = 'Mass Range (min, max, interval) ')   # update field to 'Mass Range (...)'
                self.mass_max.grid(row=3, column=2,sticky='news')                   # make entry for max
                self.mass_interval.grid(row=3, column=3,sticky='news')              # make entry for interval
                self.entries[0][1].delete(0, END)                                   # change default parameter in first fieild
                self.entries[0][1].insert(0,default_min)                            # to default min
                tip = Hovertip(self.mass_field_ent, 'Specify minimum mass (Da) of interest.\nEx: Entering 1 in (1,9,2) yields\nthe mass range [1,3,5,7,9].', hover_delay=100)
            if self.mass_range.get() == 0:              # if "Search by mass range" not selected:
                self.mass_field.config(text = 'Masses ')    # update field to 'Masses'
                self.mass_max.grid_forget()                 # remove entry for max
                self.mass_interval.grid_forget()            # remove entry for interval
                self.entries[0][1].delete(0, END)           # change default parameter in first field
                self.entries[0][1].insert(0,default_masses) # to default masses
                tip = Hovertip(self.mass_field_ent, 'List masses of interest (Da). \nSeparate each mass with either \na space or a comma and a space.', hover_delay=100)

        # In Static Mode, create buttons and entry boxes
        if self.search_optn.get() == "Static":
            for child in self.entry_frame.winfo_children():
                child.destroy()
            for child in self.mass_tol_frame.winfo_children():
                child.destroy()
            for child in self.mass_range_frame.winfo_children():
                child.destroy()
 
            #need to re-create destroyed frames
            self.entry_frame = tk.Frame(self)
            self.entry_frame.grid(row=3, column = 0, sticky='w')
            self.mass_tol_frame = tk.Frame(self)
            self.mass_tol_frame.grid(row=99, column = 0, sticky='w')
            
            self.bind("<Motion>",lambda event, w=self.entry_frame: resizer(event,w))

            # input fields in Static Mode. the 'i' variable tells grid where to start displaying the entries
            s_fields = ('Start scan', 'End scan', 'Start ret. time', 'End ret. time', 'Masses', 'Mass Error Tolerance')
            i=3

            # for each entry in Static Mode, enter the field name and entry as a pair into SearchParams.entries
            for field in s_fields:
                if field == 'Mass Error Tolerance':
                    lab = tk.Label(self.mass_tol_frame, width=len(field)+5, text=field, anchor='e')
                    ent = tk.Entry(self.mass_tol_frame, width=8)
                    lab.grid(row=0, column=0,sticky='w')
                    ent.grid(row=0, column=1,sticky='w')
                    ppm_but = tk.Radiobutton(self.mass_tol_frame, text="ppm", variable=self.v_but, value=1)
                    da_but = tk.Radiobutton(self.mass_tol_frame, text="Da", variable=self.v_but, value=2)
                    ppm_but.grid(row=0, column=2,sticky='w')
                    da_but.grid(row=0, column=3,sticky='w')
                else:
                    lab = tk.Label(self.entry_frame, width=len(field)+5, text=field, anchor='w')
                    ent = tk.Entry(self.entry_frame, width=8)
                    lab.grid(row=i, column=0,sticky='w')
                    ent.grid(row=i, column=1,sticky='w')                
                ent.bind("<Key>", lambda event, ent=ent: resizer(event,ent))
                mass_tolerance_tip= Hovertip(ent, 'Specify mass error tolerance.\nEither ppm or Da', hover_delay=100)
                SearchParams.entries.append((field, ent))
                i+=1

            # a tip pops up when the user hovers over each entry field
            scan_start_tip = Hovertip(SearchParams.entries[0][1], 'Specify starting scan number of range to be queried', hover_delay=100)
            scan_end_tip = Hovertip(SearchParams.entries[1][1], 'Specify ending scan number of range to be queried', hover_delay=100)
            ret_start_tip = Hovertip(SearchParams.entries[2][1], 'Specify starting retention time (s) to be queried', hover_delay=100)     
            ret_end_tip = Hovertip(SearchParams.entries[3][1], 'Specify ending retention time (s) to be queried', hover_delay=100)       
            masses_tip = Hovertip(SearchParams.entries[4][1], 'List masses (Da) of interest. \nSeparate each mass with either \na space or a comma and a space.', hover_delay=100)
            mass_tolerance_tip= Hovertip(SearchParams.entries[5][1], 'Specify mass error tolerance. \nEither ppm or Da.', hover_delay=100)

            # Inserts default parameters into entries (Static mode)
            self.entries[0][1].insert(0,default_scan_start)     # Start scan
            self.entries[1][1].insert(0,default_scan_end)       # End scan
            self.entries[2][1].insert(0,default_ret_start)      # Start ret. time
            self.entries[3][1].insert(0,default_ret_end)        # End ret. time
            self.entries[4][1].insert(0,default_masses)         # Masses
            self.entries[5][1].insert(0,default_tolerance)      # Mass Tolerance (ppm or Da)
            
        # In Dynamic Mode, recreate buttons and entry boxes
        if self.search_optn.get() == "Dynamic":
            for child in self.entry_frame.winfo_children():
                child.destroy()
            for child in self.mass_tol_frame.winfo_children():
                child.destroy()
            for child in self.mass_range_frame.winfo_children():
                child.destroy()

            #need to re-create destroyed frames
            self.mass_range.set(0)                                # sets "search by mass range" off by default
            self.mass_range_frame = tk.Frame(self)
            self.mass_range_frame.grid(row=2, column=0, sticky='w')
            self.entry_frame = tk.Frame(self)                     # populates frame with dynamic mode input
            self.entry_frame.grid(row=3, column = 0, sticky='w')
            self.mass_tol_frame = tk.Frame(self)
            self.mass_tol_frame.grid(row=99, column = 0, sticky='w')

            self.bind("<Motion>",lambda event, w=self.entry_frame: resizer(event,w))

            # These lists contain only fields that are NOT affected by 'Search by mass range'
            s_fields = ('Mass Error Tolerance',)                 # fields that appear in static mode that are not affected by Sbmr
            d_fields = ('Start ret. time', 'End ret. time')     # fields that appear in dynamic mode that are not affected by Sbmr

            # Search by mass range button
            self.mass_range_btn = tk.Checkbutton(self.mass_range_frame, text="Search by mass range: ", variable=self.mass_range, command=(lambda : change_range(self, self.mass_field_ent)))
            self.mass_range_btn.grid(row=0,column=1, columnspan=2, sticky='w')
            button_tip = Hovertip(self.mass_range_btn, 'Specify masses of interest by\nmin, max, and interval size (Da)\n(rather than providing a list)', hover_delay=100)

            # 'Masses' field either serves as a mass list OR the min mass for search by mass range
            self.mass_field = tk.Label(self.mass_range_frame, width=25, text='Masses', anchor='w')
            self.mass_field_ent = tk.Entry(self.mass_range_frame, width=len("".join(map(str,default_masses)))+len(default_masses))
            self.mass_field.grid(row=3, column=0,sticky='news')
            self.mass_field_ent.grid(row=3, column=1,sticky='w')
            self.mass_field_ent.bind("<Key>", lambda event, ent=self.mass_field_ent: resizer(event,ent))
            tip = Hovertip(self.mass_field_ent, 'List masses of interest (Da). \nSeparate each mass with either \na space or a comma and a space.', hover_delay=100)

            # 'Max Mass' and 'Mass Interval' fields for search by mass range
            self.mass_max = tk.Entry(self.entry_frame, width=8)
            max_tip = Hovertip(self.mass_max, 'Specify maximum mass of interest.\nEx: Entering 9 in (1,9,2) yields\nthe mass range [1,3,5,7,9].', hover_delay=100)
            self.mass_interval = tk.Entry(self.entry_frame, width=8) #make entry, but don't show until needed
            interval_tip = Hovertip(self.mass_interval, 'Specify interval for masses of interest.\nEx: Entering 2 in (1,9,2) yields\nthe mass range [1,3,5,7,9].', hover_delay=100)

            # Append info from 3 entries to SearchParams.entries
                # entries = [(masses list or min mass, mass max or None, mass_interval or None)]
                # where 2nd and 3rd elements contain None if in search by mass range
            SearchParams.entries.append(('Masses',self.mass_field_ent,self.mass_max,self.mass_interval))
            
            i=4 
            for field in s_fields:
                if field == 'Mass Error Tolerance':
                    lab = tk.Label(self.mass_tol_frame, width=len(field)+5, text=field, anchor='e')
                    ent = tk.Entry(self.mass_tol_frame, width=8)
                    lab.grid(row=0, column=0,sticky='news')
                    ent.grid(row=0, column=1,sticky='w')
                    ppm_but = tk.Radiobutton(self.mass_tol_frame, text="ppm", variable=self.v_but, value=1)
                    da_but = tk.Radiobutton(self.mass_tol_frame, text="Da", variable=self.v_but, value=2)
                    ppm_but.grid(row=0, column=2,sticky='w')
                    da_but.grid(row=0, column=3,sticky='w')
                else:
                    lab = tk.Label(self.entry_frame, width=len(field)+5, text=field, anchor='e')
                    ent = tk.Entry(self.entry_frame, width=8)
                    lab.grid(row=i, column=0,sticky='news')
                    ent.grid(row=i, column=1,sticky='w')
                ent.bind("<Key>", lambda event, ent=ent: resizer(event,ent))
                mass_tolerance_tip= Hovertip(ent, 'Specify mass tolerance.\nEither ppm or Da', hover_delay=100)
                SearchParams.entries.append((field, ent))
                i+=1

            space = tk.Label(self.entry_frame, width=20, text='\n', anchor='w').grid(row=i, column=0, sticky='w')
            i+=1
            count = 0
            # Build entries & fields for Start & End Ret. Time (per file)
            for file in SearchParams.abrv_filenames:
                # build and place labels for each file
                lab = tk.Label(self.entry_frame, width=len(App.shortened_filenames[count]), text=str(App.shortened_filenames[count]), anchor='w')
                lab.grid(row=i, column=0,columnspan=3,sticky='news')
                SearchParams.dynamic_entries.append(file)       
                j=2
                for field in d_fields:
                    lab = tk.Label(self.entry_frame, width=len(field), text=field, anchor='w')
                    ent = tk.Entry(self.entry_frame, width=8)
                    lab.grid(row=i, column=j, columnspan=1, sticky='news')
                    ent.grid(row=i, column=j+1, sticky='w')
                    if field == 'Start ret. time':
                        ret_start_tip = Hovertip(ent, 'Specify the starting retention time (s) to be queried\n' + file, hover_delay=100)
                    else:
                        ret_end_tip = Hovertip(ent, 'Specify the ending retention time (s) to be queried\n' + file, hover_delay=100)
                    SearchParams.dynamic_entries.append((field, ent))
                    j+=2
                i+=1
                count+=1

            # Inserts default parameters into entries (Dynamic mode)
            # Masses / Mass Range and Mass Tolerance
            SearchParams.entries[0][1].insert(0,default_masses)         # since search by mass range is off by default, show mass list by default (changes to min mass in change_range())
            SearchParams.entries[0][2].insert(0,default_max)
            SearchParams.entries[0][3].insert(0,default_interval)
            SearchParams.entries[1][1].insert(0,default_tolerance)
            # Start and End Ret. Times (per file)
            n = 0
            while (n < len(SearchParams.dynamic_entries)):              # going through every element of dynamic_entries, which has 3 elements per uploaded file
                start = SearchParams.dynamic_entries[n+1]               # every second element contains ('Start ret. time', Entry)
                end = SearchParams.dynamic_entries[n+2]                 # every third element contains ('End ret. time', Entry)
                start[1].insert(0, default_ret_start)                   # insert default value into entry
                end[1].insert(0, default_ret_end)                       # insert default value into entry
                n += 3                                                  # move on to next file


    # PROCESS_DECONV_FILES(): Checks for search parameter errors (no files, empty search params, conflicts with mass interval and mass tolerance) before processing (and tracking progress on) input files according to user-given search parameters.
    def process_deconv_files(self):
        # timing start
        start_time = timeit.default_timer()
                
        # Start assuming no error
        # Errors occur if there are no files to process or any empty search parameters in Static or Dynamic mode
        error = False

        # No files uploaded (nothing to process)
        if len(App.deconv_filearray) == 0:
            mb.showerror("Warning", "There are no files to process.")

        # Static Mode
        # Warning: Empty search parameter field
        if self.search_optn.get() == "Static":
            for entry in SearchParams.entries:
                if len(entry[1].get()) == 0:
                    mb.showerror("Warning", "All search parameter fields must be filled out.")
                    error = True
                    break

        # Dynamic Mode
        if self.search_optn.get() == "Dynamic":
            # Warning: Empty 'Masses'/'Min Mass' or 'Mass Tolerance' entries 
            # TO DO: FIX ERROR where this only checks first entry, and not the max or interval entries
                # entries = [('Masses', Entry obj, Entry obj, Entry obj), ('Mass Tolerance (Da)', Entry object)]
                # where the Entry objects contain mass list / min mass, max mass, mass interval, and mass tolerance, respectively
            for entry in SearchParams.entries:
                if len(entry[1].get()) == 0:
                    mb.showerror("Warning", "Please fill out all mass information.")
                    # mb.showerror("Warning", "All search parameter fields must be filled out.")
                    error = True
                    break
            
            # Warning: Overlap between 'Mass Interval' and 'Mass Tolerance'
            # The mass interval must be more than twice as large as the mass tolerance, otherwise one datapoint may be counted towards two neighboring masses of interest. Once we have the above issue fixed we can put this in the same for loop as checking the entries above and won't need  so many if statements.
            if self.mass_range.get() == 1:
                if (float(SearchParams.entries[1][1].get()) >= math.ceil(float(SearchParams.entries[0][3].get())/2)):
                    mb.showerror("Warning", "Overlap between mass interval and mass tolerance.\nInterval must be more than twice as large as tolerance.")
                    error = True

            # Warning: Empty 'Start Ret. Time' or 'End Ret. Time' entries
                # dynamic_entries = ['filename1.msalign', ('Start ret. time', Entry object), ('End ret. time', Entry object), ...]
                # where the entries contain the start and end retention times, respectively.
            # I think this search for errors will get slightly simpler if we restructure this list to remove the file names and fields (just contain the entries, which are what we work with here). I just have to check and make sure we don't use the file names in graphing, or if we do, that we can't do it another way.
            for entry in range(len(SearchParams.dynamic_entries)):              # if restructure, directly access entry objs like above
                if len(SearchParams.dynamic_entries[entry]) == 2:               # if restructure, switch 2 to 1 here
                    if len(SearchParams.dynamic_entries[entry][1].get()) == 0:
                        mb.showerror("Warning", "Please fill out min and max retention times for all files.")
                        error = True
                        break

        # If there are files to process & no errors, process msalign files according to search parameters
        if len(App.deconv_filearray) > 0 and error == False:
            # Create loading box
            self.loading = tk.Label(self, textvariable=self.e)
            self.loading.grid(column=0, row=2, columnspan=1, sticky="w")
            # Process each file, updating progress in between
            for i in range(1,len(App.deconv_filearray),2):
                self.update_progress((i/len(App.deconv_filearray)))
                SearchParams.process(self,App.deconv_filearray[i])
            # When all the calculations are complete, destroy loading page & move to Output page
            self.loading.destroy()
            self.controller.show_frame(QuantOutput)

        # timing end
        total_time = timeit.default_timer() - start_time
        # Output timing data to diff files to see static vs. dynamic performance (should just depend on size of 'Masses' list vs. list created by mass range)
        if self.search_optn.get() == "Static":
            print('SearchParams \t process_static_deconv_files \t', total_time)
            # with open("timing_0_process_static_deconv_files.csv", "a") as out_file:    # timing output file for testing
                # out_file.write(str(total_time))
                # out_file.write("\n")
        if self.search_optn.get() == "Dynamic":
            print('SearchParams \t process_dynamic_deconv_files \t', total_time)
            # with open("timing_0_process_static_dynamic_files.csv", "a") as out_file:    # timing output file for testing
                # out_file.write(str(total_time))
                # out_file.write("\n")

        
    # UPDATE_PROGRESS(): updates progress as deconv files are being processed. This function updates the progress bar. In case of any progress-specific errors
    # (the progress calculation yields an unexpected and undecipherable result) the progress bar displays an error message, progress resets to 0, and the upload continues.
    # This function is called for each input file alongside process() in process_deconv_files().
    def update_progress(self,progress):
        # timing start
        start_time = timeit.default_timer()
        
        barLength = 15          # modify to change the length of the progress bar
        status = ""

        # if progress is int, convert to float
        if isinstance(progress, int):
            progress = float(progress)

        # if progress is not float, display 'error' for status, reset progress to 0, and continue
        if not isinstance(progress, float):
            progress = 0
            status = "error: progress var must be float\r\n"

        # if progress is negative, display 'Halt' for status, reset progress to 0, and continue
        if progress < 0:
            progress = 0
            status = "Halt...\r\n"

        # if progress is 1, upload is done
        if progress >= 1:
            progress = 1
            status = "Done...\r\n"

        # update progress bar
        block = int(round(barLength*progress))
        progresstext = "\rLoading (%): [{0}] {1}% {2}".format( "#"*block + "-"*(barLength-block), int(progress*100), status)
        self.e.set(progresstext)

        # timing end
        total_time = timeit.default_timer() - start_time
        print('SearchParams \t update_progress \t\t', total_time)
        # with open("timing_0_update_progress.csv", "a") as out_file:         # timing output file for testing
            # out_file.write(str(total_time))
            # out_file.write("\n")


    # PROCESS(): takes a file as input and either converts the tsv file into a dataframe or reads the entire msalign file as a string, separates the information in the string and sorts appropriate info
    # into dataframe. Regardless of the file input, the dataframe created will be organized exactly the same for follow up processing.
    def process(self,filename):
        # timing start
        start_time = timeit.default_timer()
        
        temp_array = [0,0,0,0]  # temp array for each scan
        ms1_ions = []           # last in scan array
        convert = []            # file data fills here as it is being converted
        lines = []              # lines/rows of data fill here
        req_columns = ['Index','ScanNum','RetentionTime','MonoisotopicMass','SumIntensity'] #these are both the requiered columns needed from the tsv file and the dataframe columns once the msalign file gets read
        df_array = []

        # Read input file as one massive string
        root_ext = os.path.splitext(filename)
        if root_ext[1] == '.msalign':
            with open(filename) as fp:
                data = fp.read()

            # Read through the string obtained from the file and sort it into "lines". 
            # Lines consist of the data between each comma or newline character. 
            for i in data:
                if i == ',' or i == '\n':
                    link = "".join(convert)
                    link.strip()
                    if len(link) > 1:
                        lines.append(link)
                        convert = []
                else:
                    convert.append(i)

          # Extract numerical data from lines using rmv_chars(), defined above. 
            # By inspection, the data in the input (.msalign) files is in the form:
                # BEGIN IONS
                # ID=int
                # SCANS=int
                # RETENTION_TIME=float
                # ACTIVATION= 
                # float float int (ion 1 data, represented as monoisotopic mass, intensity, max charge state, (EnvCNN score?), respectively)
                # float float int (ion 2 data)
                # ...
                # END IONS
                #
                # (repeat above)
                # !!! VERY IMPORTANT: newer msalign files from TopFD have a fourth column that represents an EnvCNN quality score !!!
            # So we use these headers (and a temp array) to sort the appropriate data to be converted into the dataframe, msalign_df.
            while (len(lines)-1)>0:
                text = str(lines[0])
                count = 0 # older msalign files have 3 columns, newer msalign files have 4 so I need to count them in order to processess correctly
                if text.startswith('ID='):
                    temp_array[0] = int(App.rmv_chars(text))        # append id no. to temp array
                    self.update()
                elif text.startswith('SCANS='):                 # append scan no. to temp array
                    temp_array[1] =  int(App.rmv_chars(text))
                elif text.startswith('RETENTION_TIME='):        # append ret. time to temp array
                    temp_array[2] = temp_ret = float(App.rmv_chars(text))
                elif text[0].isdigit():
                    while lines[0]!='END IONS':
                        ms1_ions = np.append(ms1_ions,[float(s) for s in lines[0].split("\t")])     # append ions to ms1_ions
                        del lines[0]
                        if count == 0:
                            count = len(ms1_ions)
                    ms1_ions = np.reshape(ms1_ions,(int(len(ms1_ions)/count),count))    # each element now contains 3 or 4 pieces of data for an ion: monoisotpic mass, intensity and max charge state,  *EnvCNN score*)
                    temp_array[3] = ms1_ions
                    for ion in ms1_ions:
                        df_array.append([temp_array[0],temp_array[1],temp_array[2],ion[0],ion[1]])
                    temp_array = [0,0,0,0]
                    ms1_ions = []
                del lines[0]
            msalign_df = pd.DataFrame(df_array, columns=req_columns) 
            
            # timing end (end timing before calling next function)
            total_time = timeit.default_timer() - start_time
            print('SearchParams \t process \t\t\t', total_time)
            # with open("timing_0_progress.csv", "a") as out_file:        # timing output file for testing
                # out_file.write(str(total_time))
                # out_file.write("\n")

            # Perform mass selection (defined below) on processed dataframe
            SearchParams.mass_selection(self,msalign_df)

        elif root_ext[1] == '.tsv':
            tsv_df = pd.read_csv(filename,usecols=req_columns,sep='\t')
            tsv_df = tsv_df.sort_values(by=['MonoisotopicMass'])
            self.update()
            SearchParams.mass_selection(self,tsv_df)

        else:
             mb.showerror("Warning","File extension not .msalign or .tsv. Closing program.")
             sys().exit
        
    # MASS_SELECTION(): given deconv dataframe array from process(), where
    # decon_df = ['Index','ScanNum','RetentionTime','MonoisotopicMass','SumIntensity']
        # gathers parameter data from user input
        # converts all mass data (idx, val) from string to float
        # forms found_masses, masses, tolerance arrays from mass data & parameters, where 
            # found_masses = 
            # masses = 
            # tolerance = 
    def mass_selection(self,decon_df):
        # timing start
        start_time = timeit.default_timer()

        masses = []
        input_masses = [0]      # set to an integer because string values will be entered by user. this makes sure the user entered multiple values separated by a space or comma.

        # Gather entries from parameters in Static Mode
        if self.search_optn.get() == "Static":
            scan_min = float(SearchParams.entries[0][1].get())          # Start Scan
            scan_max = float(SearchParams.entries[1][1].get())          # End Scan
            retention_min = float(SearchParams.entries[2][1].get())     # Start Ret. Time
            retention_max = float(SearchParams.entries[3][1].get())     # End Ret. Time
            input_masses = str(SearchParams.entries[4][1].get())        # Masses
            mass_tolerance = float(SearchParams.entries[5][1].get())    # Mass Tolerance (Da or ppm)

        # OR Gather entries from parameters in Dynamic Mode
        if self.search_optn.get() == "Dynamic":
            scan_min = 0                                                # Question: Do we want an entry for this per file?
            scan_max = 30000                                            # same here
            retention_min = float((SearchParams.dynamic_entries[SearchParams.dynamic_counter][1]).get())
            retention_max = float((SearchParams.dynamic_entries[SearchParams.dynamic_counter+1][1]).get())
            SearchParams.dynamic_counter+=3
            if self.mass_range.get() == 1:                              # gather 'Search by mass range' data
                min_mass = float(SearchParams.entries[0][1].get())
                max_mass = float(SearchParams.entries[0][2].get())
                mass_interval = float(SearchParams.entries[0][3].get())
                mass_tolerance = float(SearchParams.entries[1][1].get())
                input_masses = [min_mass]
                temp_mass = min_mass
                while temp_mass <= max_mass:
                    temp_mass+=mass_interval
                    input_masses.append(temp_mass)
                input_masses = str(input_masses)                        # not sure
            else:                                                       # if 'Search by masses' not selected
                input_masses = str(SearchParams.entries[0][1].get())    # only additional data to gather is
                mass_tolerance = float(SearchParams.entries[1][1].get())    # Masses and Mass tolerance (Da)

        #found_masses = []
        #graph_found_masses = []         # this array will pass on 0 intensity values for QC graphs
        graph_found_masses_df = []
        convert = []
        pool = []

        # convert string masses to float so their data can be used in calculations
        if type(input_masses[0]) == str:                # input_masses = list of masses as strings, why if?
            masses = []
            for idx, val in enumerate(input_masses):
                if val == ',' or val == ' ' and len(convert) !=0:
                    link = "".join(convert)
                    pool.append(float(link))
                    convert = []
                elif idx == (len(input_masses)-1) and input_masses[idx].isnumeric():
                    convert.append(input_masses[idx])
                    link = "".join(convert)
                    pool.append(link)
                elif val.isnumeric() or val == '.':
                    convert.append(val)

            for i in pool:
                masses.append(float(i))
            masses = np.array(masses) #need to convert to np for max and min searching below

        graph_req_columns=['RetentionTime','QueriedMass','MonoisotopicMass','SumIntensity','MassTolerance']
        trimmed_decon_df = decon_df[(decon_df['MonoisotopicMass']>masses.min()-1000) & (decon_df['MonoisotopicMass']<masses.max()+1000)] #huge savings in time limiting search space to max and min inputed mass values
        for i, row in trimmed_decon_df.iterrows():
            for mass in masses:
                mass_tol = mass_tolerance
                if self.v_but.get() == 1: #ppm search
                    mass_tol = (mass*mass_tolerance)/10**6
                if (mass - mass_tol) <= row['MonoisotopicMass'] <= (mass + mass_tol):
                    if scan_min <= row['ScanNum'] <= scan_max and retention_min <= row['RetentionTime'] <= retention_max:
                        temp_array = []
                        temp_array.extend((row['RetentionTime'],row['MonoisotopicMass'],row['SumIntensity']))
                        graph_found_masses_df.append([row['RetentionTime'],mass,row['MonoisotopicMass'],row['SumIntensity'],mass_tol]) #df 
                
        graph_found_masses_df = pd.DataFrame(graph_found_masses_df, columns=graph_req_columns)
        
        #need to add an empty all-zero value for any mass searched for, but not found
        for mass in masses:
            if mass not in graph_found_masses_df['QueriedMass'].values:
                temp_df = {'RetentionTime': 0,'QueriedMass': mass,'MonoisotopicMass': 0,'SumIntensity': 0,'MassTolerance': 0}
                graph_found_masses_df.loc[len(graph_found_masses_df)] = temp_df
        graph_found_masses_df=graph_found_masses_df.sort_values(by=['QueriedMass', 'RetentionTime']).reset_index(drop=True) #df

        # Perform mass quantification on found masses
        SearchParams.mass_quantification(self,graph_found_masses_df) #found_masses, masses, mass_tolerance,

        # Build qc graph result from found masses, masses, and mass tolerance arrays determined above
        QCGraphs.total_qc_graph_array_df.append(graph_found_masses_df)
        
        # timing end
        total_time = timeit.default_timer() - start_time
        print('SearchParams \t mass_selection \t\t', total_time)
        # with open("timing_0_mass_selection.csv", "a") as out_file:      # timing output file for testing
            # out_file.write(str(total_time))
            # out_file.write("\n")


    # MASS_QUANTIFICATION(): Uses found_masses, masses, and mass_tolerance arrays to calculate the intensity of each mass of interest (using values from within the specified tolerance), then sums all the intensities and pairs each mass with its percent of the total intensity, to yield the abundance of each mass. This data is saved to App.processedfilearray.
    # Also contains warning for no masses found (per file).
    def mass_quantification(self,graph_found_masses_df): #found_masses,masses,mass_tolerance,
        # timing start
        start_time = timeit.default_timer()

        # Initialize intensity information to 0 and empty before performing calculations
        percent_intensities = []
        summed_intensities_df = graph_found_masses_df.groupby('QueriedMass')['SumIntensity'].sum()
        total_intensity = graph_found_masses_df['SumIntensity'].sum()
        for idx,i in summed_intensities_df.items():
            if total_intensity > 0:
                percent_intensities.append([idx, i/total_intensity*100])
            else:
                percent_intensities.append([idx, 0])
        #if total_intensity > 0:
        App.processed_filearray.append(percent_intensities)                 #((mass1,percent1),(mass2,percent2),...)
        # If total_intensity nonpositive, cannot calculate percentages
        #else:
        #    mb.showerror("Warning","No masses found for one or more of the inputted files.")
        #    self.controller.show_frame(SearchParams)

        # timing end
        total_time = timeit.default_timer() - start_time
        print('SearchParams \t mass_quantification \t\t', total_time)
        # with open("timing_0_mass_quantification.csv", "a") as out_file:             # timing output file for testing
            # out_file.write(str(total_time))
            # out_file.write("\n")


"""
QUANT OUTPUT
Takes calculated data from SearchParams to form output graphs & csv files.

"""
class QuantOutput(tk.Frame):
    averaged_data = [] #[[averages],[stdev]]

    def __init__(self, parent, controller):                                 # creating buttons for quant output: output, calc data, search params
        # Quant Output Page creation & initialization
        tk.Frame.__init__(self, parent)
        label = tk.Label(self, text="Results Output", font=LARGE_FONT)
        label.grid(column=0, row=0, sticky='w')

        # Calculate Avg & Std Dev Button
        button1 = tk.Button(self, text="Calculate Avg and Stdev", bg="white", fg="black",
                            command=lambda: self.analyze_data())
        button1.grid(column=0, row=1, sticky='w')
        calculations_tip = Hovertip(button1, 'Perform calculations based on entered search parameters', hover_delay=500)

        # Search Parameters Button
        button2 = ttk.Button(self, text="Search Parameters",
                            command=lambda: controller.show_frame(SearchParams))
        button2.grid(column=1, row=1, sticky='w')
        searchparams_tip = Hovertip(button2, 'Return to Search Parameters page', hover_delay=500)

        self.controller = controller


    # ANALYZE_DATA(): given data in processed_filearray and expgroup, groups data
    # then calculates avg and std deviation. this data is appended to the arrays
    # grouped_data and calc_avg_stdev, the latter of which is passed to QC Graphs
    # (to form error bars).
    def analyze_data(self):
        # timing start
        start_time = timeit.default_timer()

        # arrays we are filling in this function
        temp_array = []
        restructured_data_df = []

        # arrays that contain the data to be used in calculations
        processed_filearray_temp = App.processed_filearray
        expgroup_temp = App.expgroup

        # restructure data into a new array [[group1, [data1]], [group2, [data2]],...]
        # where [data] is [mass, intensity]
        if len(expgroup_temp) == 0 or len(processed_filearray_temp) == 0:
            mb.showerror("Warning","No masses found for one or more of the inputted files.")
            self.controller.show_frame(SearchParams)
        else:
            for i in range(len(expgroup_temp)):
                temp_array = [expgroup_temp[i],processed_filearray_temp[i]]
                for j in processed_filearray_temp[i]:
                    restructured_data_df.append([expgroup_temp[i],j[0],j[1]])

        req_columns = ['GroupName','QueriedMass','RelAbundance(%)']
        grouped_data_df = pd.DataFrame(restructured_data_df, columns=req_columns)

        grouped_data_avgstdev_df = pd.DataFrame()
        grouped_data_avgstdev_df['mean'] = grouped_data_df.groupby(['GroupName','QueriedMass']).mean()
        grouped_data_avgstdev_df['stdev'] = grouped_data_df.groupby(['GroupName','QueriedMass']).std()
        grouped_data_avgstdev_df['upper'] = grouped_data_avgstdev_df['mean']+grouped_data_avgstdev_df['stdev']
        grouped_data_avgstdev_df['lower'] = grouped_data_avgstdev_df['mean']-grouped_data_avgstdev_df['stdev']
        grouped_data_avgstdev_df=grouped_data_avgstdev_df.reset_index()
        QCGraphs.grouped_data_avgstdev_df = grouped_data_avgstdev_df
        

        # the final product [[group1, [avg,stdev]],[group2, [avg,stdev],...]
        # will be sent to the same variable in the QCGraphs for graph creation
        self.output_quantification_file()
        self.controller.show_frame(QCGraphs)

        # timing end
        total_time = timeit.default_timer() - start_time
        print('QuantOutput \t analyze_data \t\t\t', total_time)
        # with open("timing_0_analyze_data.csv", "a") as out_file:                # timing output file for testing
            # out_file.write(str(total_time))
            # out_file.write("\n")

    # OUTPUT_QUANTIFICATION_FILE(): 
    def output_quantification_file(self):
        # timing start
        start_time = timeit.default_timer()

        # qc graphs will be named by date (up to second)
        date = datetime.today().strftime('%Y%m%d_%H%M%S')   # calculate exact time/date
        filenames = []
        a = []

        # check if any files were uploaded (if not, Error generating output)
        if (len(App.processed_filearray)) == 0:
            mb.showerror("Warning", "Based on your search criteria, no results were generated.")
        else:
            # name output files by date, calculated above
            # output data (SearchParams.abrv_filenames, App.processed_filearray) to file
            filename = "MS1_quantification"+"_"+date+".csv"
            with open(filename,"a") as out_file:
                for i in range(len(SearchParams.abrv_filenames)):
                    out_file.write(str(SearchParams.abrv_filenames[i])+"\n")
                    for j in App.processed_filearray[i]:
                        x,y = str(j[0]),str(j[1])
                        temp = (x+','+y+',')
                        out_file.write(temp)
                    out_file.write(" \n")

        # timing end
            total_time = timeit.default_timer() - start_time
            print('QuantOutput \t output_quantification_file \t', total_time)
            # with open("timing_0_output_quantification_file.csv", "a") as out_file:      # timing output file for testing
                # out_file.write(str(total_time))
                # out_file.write("\n")


"""
QUALITY CONTROL GRAPHS
This class contains the actual QC graphs so its only function is makegraph().
"""
class QCGraphs(tk.Frame):
    total_qc_graph_array_df = [] #storage of dataframes
    
    def __init__(self, parent, controller):
        # QC Graphs Page creation & initialization
        tk.Frame.__init__(self, parent)
        label = tk.Label(self, text="Results with quality control graphs", font=LARGE_FONT)
        label.grid(column=0, row=0, sticky='w')

        # Show QC Graphs Button
        button1 = tk.Button(self, text="Show QC Graphs", bg="white", fg="black",
                            command=lambda: self.makegraph())
        button1.grid(column=0, row=1, sticky='w')
        graphs_tip = Hovertip(button1, 'Show output graphs', hover_delay=500)         # this needs a better message

        # Search Parameters Button
        button2 = ttk.Button(self, text="Search Parameters",
                            command=lambda: controller.show_frame(SearchParams))
        button2.grid(column=1, row=1, sticky='w')
        searchparams_tip = Hovertip(button2, 'Return to Search Parameters page', hover_delay=500)
        
        # Search Parameters Button
        button3 = ttk.Button(self, text="Exit Program",
                            command=lambda: sys.exit())
        button3.grid(column=2, row=1, sticky='w')
        exit_tip = Hovertip(button3, 'Stop the script', hover_delay=500)

        # New Analysis Button
        #button3 = tk.Button(self, text="New Analysis",
        #                    command=lambda: controller.new_analysis(), state=DISABLED)      # temporarily disabled
        #button3.grid(column=2, row=1, sticky='w')
        #newanalysis_tip = Hovertip(button3, 'Currently unavailable', hover_delay=500)      # temporarily unavailable


    # MAKEGRAPH(): uses data from calculations above to form QC Graphs,
    # displayed in browser (will change to display in App later).
    def makegraph(self):
        # timing start
        start_time = timeit.default_timer()

        # max_graphs = max number of graphs is total number of input files
        # plus one for main graph w data averaged across conditions
        # max_groups = number of groups/conditions which will affect the size of each bar in the histogram so that they do not overlap
        max_graphs = len(QCGraphs.total_qc_graph_array_df)
        max_groups = QCGraphs.grouped_data_avgstdev_df['GroupName'].nunique()
        if max_groups < 5:
            group_bar = 2
        elif 6 < max_groups < 12:
            group_bar = 1
        else:
            group_bar = 0.5

        # change font size for QC graphs depending on number of masses
        if max_graphs < 7:
            graph_font_size = '16px'
            glyph_size = 20
        elif 8 <= max_graphs <= 12:
            graph_font_size = '12px'
            glyph_size = 10
        elif max_graphs > 13:
            graph_font_size = '10px'
            glyph_size = 6
        else:
            graph_font_size = '12px'
            glyph_size = 20

        # data is organized in chart by mass, abundance, and condition
        TOOLTIPS = [("Mass: ", "@QueriedMass{0.00}"),("Abundance (%): ", "@mean{0.00}"),("Condition: ", "@GroupName"),]
        TOOLTIPSQC = [("Ret. Time", " @RetentionTime{0.000}"),("Intensity", " @TotalIntensity{0.000}"),("Searched Mass", " @QueriedMass{0.000}"),("Found Mass(es)"," @MonoisotopicMassesFound")]

        # averaged plot displayed first
        averaged_plot = figure(title="Averaged MS1 Data per Condition", x_axis_label="Mass", y_axis_label="Abundance (%)", tools="pan,box_zoom,wheel_zoom,reset,undo,save", active_drag="box_zoom",
                               active_scroll="wheel_zoom", x_range=((QCGraphs.grouped_data_avgstdev_df["QueriedMass"].min()-100), (QCGraphs.grouped_data_avgstdev_df["QueriedMass"].max()+100)),tooltips=TOOLTIPS, height=500)

        # color selection variable used to change color AND
        # serves as the off-set for grouped data bar graphs
        color_selection = 0
        offset = 0.25
        for i,groupname in QCGraphs.grouped_data_avgstdev_df.groupby('GroupName'):
            print('groupname',groupname)
            color_palette = Category20c[20]                             # color palette 
            if color_selection == 19:                                   # once all colors used, reset & cycle back
                color_palette = Category20b[20]
                color_selection = 0

            data = groupname
            source = ColumnDataSource(data=data)

            averaged_plot.vbar(x=dodge("QueriedMass",offset), width=group_bar, bottom=0, top="mean",source=source,
                                legend_label="Condition: "+str(groupname['GroupName'].iloc[0]), color=color_palette[color_selection])
            averaged_plot.add_layout(Whisker(source=source, base=dodge("QueriedMass",offset), upper="upper", lower="lower"))

            color_selection+=1
            
            if max_groups < 5:
                offset+=2
            elif 6 < max_groups < 12:
                offset+=1
            else:
                offset+=0.5          

        tab = []    
        for idx,graph_df in enumerate(QCGraphs.total_qc_graph_array_df):
            ret_time = []
            mass = []
            temp_intensity=0
            mass_coord = []
            monoisotopicmasses = []
            offset = .25
            
            color_selection = 0
            color_palette = Category20c[20]

            filename = str(SearchParams.abrv_filenames[idx])[::-1]        # takes the uploaded file path as a string, reverses it, and saves this to the variable filename
            for i in range(len(filename)):
                if filename[i] == '/':
                    ext_filename = filename[0:i][::-1]    # takes the reversed file path up to the first '/', reversed, and saves this to the variable ext_filename
                    break

            p = figure(title="QC Graph for "+ext_filename,x_axis_label="Ret. time(s)", tools="pan,box_zoom,wheel_zoom,reset,undo,save",
                       active_scroll="wheel_zoom",y_axis_label="Intensity", tooltips=TOOLTIPSQC, active_drag="box_zoom", height=500)

            #for deconvoluted files that have multiple masses that meet the search criteria in a single scan (or ret time), they need to be summed to create
            #a single mass to be plotted
            print('graph df',graph_df)
            for i, row in graph_df.iterrows():
                if i<(len(graph_df)-1):
                    if row['QueriedMass'] == graph_df.iloc[i+1]['QueriedMass'] and row['RetentionTime'] == graph_df.iloc[i+1]['RetentionTime']:
                        if temp_intensity == 0:
                            temp_intensity+= row['SumIntensity']+graph_df.iloc[i+1]['SumIntensity']
                            monoisotopicmasses.append(row['MonoisotopicMass'])
                        else:
                            temp_intensity+=graph_df.iloc[i+1]['SumIntensity']
                            monoisotopicmasses.append(row['MonoisotopicMass'])
                    else:
                        if temp_intensity == 0:
                            temp_intensity = row['SumIntensity']
                            monoisotopicmasses.append(row['MonoisotopicMass'])
                        mass=row['QueriedMass']
                        ret_time=row['RetentionTime']
                        mass_coord.append([ret_time,mass,monoisotopicmasses,temp_intensity])
                        monoisotopicmasses=[]
                        temp_intensity = 0
                #print('mass coord',mass_coord)
            df_temp_columns = ['RetentionTime','QueriedMass','MonoisotopicMassesFound','TotalIntensity']
            df_temp=pd.DataFrame(mass_coord, columns=df_temp_columns)
            #print('df_temp',df_temp)

            legend_items = []
            masses_searched = pd.unique(df_temp['QueriedMass'])
            for i in masses_searched:
                plot_df = df_temp[df_temp['QueriedMass']==i]
                color_selection+=1

                if color_selection == 19:
                    color_palette = Category20b[20]
                    color_selection = 0

                data = plot_df
                source = ColumnDataSource(data=data)

                z = p.vbar(x=dodge("RetentionTime",offset), width=0.25, bottom=0, top="TotalIntensity", color=color_palette[color_selection], source=source)
                legend_items.append(LegendItem(label=str(i)+' Da', renderers=[z]))
                offset+=0.25

            legend = Legend(items=legend_items,label_text_font_size=graph_font_size,glyph_width=5,label_standoff=1,
                            glyph_height=glyph_size,padding=1,spacing=1,margin=5,label_text_line_height=0.1,label_height=0)
            p.add_layout(legend,'right')
            p.legend.click_policy="hide"
            tab_temp = TabPanel(child=p, title=ext_filename)
            tab.append(tab_temp)
            del p
            
        date = datetime.today().strftime('%Y%m%d_%H%M%S')
        filename = "pyMSVIS_output_"+date+".html"           # names graph output file by date
        output_file(filename)                               # from bokeh, outputs file containing quant graphs

        # make a graphical layout of all plots
        stylesheet = """
        :host(.bk-Tabs) .bk-header {
          font-size: 15px;
          overflow: hidden;
          border-color: orange;
        }

        .bk-tab {
            padding: 2px 3px;
            border: solid transparent;
            outline: 0;
            outline-offset: -5px;
            white-space: nowrap;
            cursor: pointer;
            text-align: center;
            max-width: 30px;
            border-color: orange;
            overflow: hidden;
            text-wrap: wrap;
        }

        .bk-tab.bk-active {
          font-size: 15px;
          max-width: 500px;
          overflow: hidden;
          background-color: orange;
        }
        """
        #Check for feature maps to display. If 'None' is selected, adjust output to maximize the QC graphs
        #Plots the graphs with the averaged plot on top and the two other graphs (histogram and feature map) next to each other below.
        feature_map = VisualizeMS1FT.make_feat_map_graphs()
        print('feature_map', feature_map)
        if len(feature_map) == 0:
            graph_layout = layout([averaged_plot],[Tabs(tabs=tab, tabs_location='above', stylesheets=[stylesheet])],sizing_mode='scale_both')
            show(graph_layout)
        else:
            graph_layout = layout([averaged_plot],[Tabs(tabs=tab, tabs_location='above', stylesheets=[stylesheet]),Tabs(tabs=VisualizeMS1FT.make_feat_map_graphs(),tabs_location='above',stylesheets=[stylesheet])],sizing_mode='scale_both')
            show(graph_layout)

        # clear out graphing data
        QCGraphs.total_qc_graph_array_df, App.processed_filearray = [],[]
        SearchParams.dynamic_counter = 1                                # need to reset counter if another search is done

        # timing end
        total_time = timeit.default_timer() - start_time
        print('QCGraphs \t makegraph \t\t\t', total_time)
        # with open("timing_0_makegraph.csv", "a") as out_file:       # timing output file for testing
            # out_file.write(str(total_time))
            # out_file.write("\n")

        # QC Graph output is last step of app but don't close app, just return
        return        
  
    
class VisualizeMS1FT():
    def make_feat_map_graphs():
        feat_map_graphs = []
        print('App.feat_map_filenames',App.feat_map_filenames)
        for idx,file in enumerate(App.feat_map_filenames):
            if os.path.isfile(file):
                df_feat_map = pd.read_csv(file,sep='\t')     # reads input file (tab delimited) into a pandas dataframe
                if df_feat_map.columns[0] == "FeatureID":    # This is the first cell in the header for .ms1ft files generated by Promex/FLASHDeconv
                    df_feat_map = df_feat_map.sort_values(by='MonoMass').set_index(keys='MonoMass')   # sets the 'MonoMass' datapoint as the key for its corresponding data, and sorts the dataframe by it
                    cols = ['ElutionLength','MaxElutionTime','MinElutionTime']
                    df_feat_map[cols] = df_feat_map[cols].multiply(60) #convert time to seconds
                                          
                    date = datetime.today().strftime('%Y%m%d_%H%M%S')   # calculate exact time/date
                    
                    output_file("pyMSVIS_output_"+date+".html", mode='inline')
                    VisualizeMS1FT.envelope_histogram(df_feat_map)                                     # forms envelope histogram out of sorted dataframe containing ms1ft file data

                    #Autopopulate istopic distribution histogram with that of the most abundant ion
                    max_abd_idx = df_feat_map['Abundance'].idxmax()
                    max_abd_C13 = df_feat_map['C13'].loc[max_abd_idx]
                    max_abd_IsoAbd = df_feat_map['IsoAbd'].loc[max_abd_idx]                    
                    initial_data = {'x': max_abd_C13, 'y':max_abd_IsoAbd}
                                    
                    source = ColumnDataSource(df_feat_map)
                    source2 = ColumnDataSource(initial_data)



                    colors = ["#75968f", "#a5bab7", "#c9d9d3", "#e2e2e2", "#dfccce", "#ddb7b1", "#cc7878", "#933b41", "#550b1d"]
                    mapper = LogColorMapper(palette=colors, low=df_feat_map['Abundance'].min(), high=df_feat_map['Abundance'].max())

                    p = figure(title="Feature Map of "+App.shortened_filenames[idx],
                               x_range=(0,df_feat_map['MaxElutionTime'].max()+1), y_range=(0,30000), #y_range=(0,df_feat_map.index.max()+1) = this will scale to largest mass identified, really impratical so i set to 30,000
                               toolbar_location="right",  x_axis_location="below",active_drag="box_zoom",active_scroll="wheel_zoom", height=300)

                    
                    iso_distrib = figure(title="Isotopic Distribution for: "+str(max_abd_idx),x_axis_label="C13", tools="pan,box_zoom,wheel_zoom,reset,undo,save",
                                        active_scroll="wheel_zoom",y_axis_label="Rel. Abundance", active_drag="box_zoom", height=200)
                     
                    z = iso_distrib.vbar(x='x', width=0.5, bottom=0, top='y', color="red", source=source2)


                    mass = p.rect(x="MinElutionTime", y="MonoMass", width='ElutionLength', height=2, source=source,
                           line_color=None, fill_color=transform('Abundance', mapper))
                    

                    p.add_tools(TapTool())

                    p.js_on_event(Tap, CustomJS(args=dict(source=source, source2=source2,title=iso_distrib.title), code="""
                        // get data source from Callback args
                        let data = Object.assign({}, source.data);
                        source2.data.x = data.C13[source.selected.indices];
                        source2.data.y = data.IsoAbd[source.selected.indices];
                        title.text = 'Isotopic Distribution for: '+(new String(data.MonoMass[source.selected.indices]));
                        source2.change.emit();
                    """)
                    )

                    color_bar = ColorBar(color_mapper=mapper, title="Log Abundance")

                    p.add_layout(color_bar, 'right')
                    p.axis.axis_line_color = None
                    p.axis.major_tick_line_color = None
                    p.xaxis.axis_label = 'Retention Time (sec)'
                    p.yaxis.axis_label = 'Monoisotopic Mass'
                    p.axis.major_label_text_font_size = "20px"
                    p.axis.major_label_standoff = 0
                    p.xaxis.major_label_orientation = 1.0
                    p.axis.axis_label_text_font_size = "20px"

                    temp = layout([p, iso_distrib])
                    tab = TabPanel(child=temp, title=App.shortened_filenames[idx])
                    feat_map_graphs.append(tab)
                elif df_feat_map.columns[0] == "Sample_ID": # This is the first cell in the header for .feature files generated by TopFD
                    if 'Monoisotopic_mass' in df_feat_map.columns: #newer TopFD uses different headers than older versions so assignments reflect column headers that are different
                        mass = 'Monoisotopic_mass'
                        max_time = 'Max_time'
                        min_time = 'Min_time'
                        cols = [min_time,max_time]
                        df_feat_map[cols] = df_feat_map[cols].multiply(60) #convert Max_time and Min_time to seconds (Elution_length already is in seconds!!!)
                    else:
                        mass = 'Mass'
                        max_time = 'Time_end'
                        min_time = 'Time_begin'
                        df_feat_map['Elution_length'] = (df_feat_map[max_time] - df_feat_map[min_time])
                    df_feat_map = df_feat_map.sort_values(by=mass).set_index(keys=mass)   # sets the 'mass' datapoint as the key for its corresponding data, and sorts the dataframe by it
                                          
                    date = datetime.today().strftime('%Y%m%d_%H%M%S')   # calculate exact time/date
                    
                    output_file("pyMSVIS_output_"+date+".html", mode='inline')

                    colors = ["#75968f", "#a5bab7", "#c9d9d3", "#e2e2e2", "#dfccce", "#ddb7b1", "#cc7878", "#933b41", "#550b1d"]
                    mapper = LogColorMapper(palette=colors, low=df_feat_map['Intensity'].min(), high=df_feat_map['Intensity'].max())
                    
                    source = ColumnDataSource(df_feat_map)

                    p = figure(title="Feature Map of "+App.shortened_filenames[idx],
                               x_range=(0,df_feat_map[max_time].max()+1), y_range=(0,30000), #y_range=(0,df_feat_map.index.max()+1) = this will scale to largest mass identified, really impratical so i set to 30,000
                               toolbar_location="right",  x_axis_location="below",active_drag="box_zoom",active_scroll="wheel_zoom", height=500)

                    mass = p.rect(x=min_time, y=mass, width='Elution_length', height=6, source=source,
                           line_color=None, fill_color=transform('Intensity', mapper))
                    
                    color_bar = ColorBar(color_mapper=mapper, title="Log Abundance")

                    p.add_layout(color_bar, 'right')
                    p.axis.axis_line_color = None
                    p.axis.major_tick_line_color = None
                    p.xaxis.axis_label = 'Retention Time (sec)'
                    p.yaxis.axis_label = 'Monoisotopic Mass'
                    p.axis.major_label_text_font_size = "20px"
                    p.axis.major_label_standoff = 0
                    p.xaxis.major_label_orientation = 1.0
                    p.axis.axis_label_text_font_size = "20px"

                    temp = layout(p)
                    tab = TabPanel(child=temp, title=App.shortened_filenames[idx])
                    feat_map_graphs.append(tab)
                else:
                    mb.showerror("File Error", "Check the feature map file as \nthe expected data structure was absent.")

        return feat_map_graphs

    # Function Name: Envelope Histogram
    # Input: Dataframe generated from ms1ft feature map
    # Output: Restructures the feature map so that the isotopic distribution can be plotted
    # Description: This function takes a list (of the data read from the input ms1ft file) and 
    def envelope_histogram(df_feat_map):
        envelope = list(df_feat_map['Envelope'])
        carb_isotope = []
        carb_abundance = []
        convert = []
        pool_c13 = []
        pool_isoabd = []
         
        for distribution in envelope:
            for idx, val in enumerate(distribution):
                if val == ',': 
                    link = "".join(convert)
                    carb_isotope.append(int(link))
                    convert = []
                elif val == ';' or idx == (len(distribution)-1): # older versions of ms1ft have Envelopes that end with ';' vs. an empty space
                    link = "".join(convert)
                    carb_abundance.append(float(link))
                    convert = []
                else:
                    convert.append(val)
            
            pool_c13.append(carb_isotope)
            pool_isoabd.append(carb_abundance)
            carb_isotope,carb_abundance = [],[]
        df_feat_map['C13'],df_feat_map['IsoAbd'] = pool_c13, pool_isoabd


print('Class\t\t Function \t\t\t Runtime (s)')
print('-------\t\t ----------\t\t\t -----------')







app = App()
app.mainloop()
